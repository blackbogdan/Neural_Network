{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Network to determine number from MINST images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot\n",
    "import scipy.special # for activation function\n",
    "import scipy.ndimage\n",
    "import os\n",
    "import glob\n",
    "from random import shuffle\n",
    "import time\n",
    "# for plotting inside notebook, not in separate window\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self, input_nodes=784, hidden_nodes=250, output_nodes=10, learning_rate=0.01):\n",
    "        self.inodes = input_nodes   # Because we have images 28x28 pixels\n",
    "        self.hnodes = hidden_nodes  # We can think of this number as representation of features that correspond to \n",
    "        # picture of number. The more hidden_nodes more features our NN can see. If we choose smaller \"hidden_nodes\"\n",
    "        # that some of features should be combined \n",
    "        self.onodes = output_nodes  # default is 10, because on images have numbers from 0 to 9\n",
    "        self.lr = learning_rate\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)  # activation fucntion 1/(1 + e**(-x))\n",
    "        self.wih = None # weights from input to hidden layers\n",
    "        self.who = None # weights from hidden to output layers\n",
    "        # we need to get weights, if they are present, or generate new random ones using normal distribution\n",
    "        # how to get oldest file: self.chrome_extension_path = max(list_of_files, key=os.path.getctime)\n",
    "        epoch_to_look_for = \"epoch_0\"  # Randomly created weights shall have this name\n",
    "        cur_dir = os.getcwd()\n",
    "        epoch_files = glob.glob(os.path.join(cur_dir, '*.npz'))\n",
    "        epoch_zero_exist = any([epoch_to_look_for in file for file in epoch_files])         \n",
    "        if epoch_zero_exist:\n",
    "            # we have file with predefined weights, let's use it\n",
    "            for file in epoch_files:\n",
    "                if epoch_to_look_for in file: \n",
    "                    self.load_weights_from_file(file)\n",
    "                    print(\"Using weights from file: {}\".format(file))\n",
    "        else:\n",
    "            # we do not have file with weights, let's generate random weights using normal distribution:\n",
    "            self.wih = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))\n",
    "            self.who = np.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "            # Gets sample of weights from a normal probablity distribution centerd around zero\n",
    "            # and with a standart deviation that is related to the nubmer of incoming links into node, 1/sqrt(number_of_nodes)\n",
    "            # where 0.0 - standard deviation from normal destribution; \n",
    "            # pow(self.onodes, -0.5) 1/sqrt(number_of_nodes); \n",
    "            # (self.hnodes, self.inodes) - determines the size. F.e. (2, 3) - matrix 2 rows 3 coulumns\n",
    "            self.save_weights_to_file('epoch_0_{}hidden_nodes_normal_distribution.npz'.format(hidden_nodes), 'normal_distribution')\n",
    "\n",
    "    def load_weights_from_file(self, file_path):\n",
    "        '''\n",
    "        Loads wih and who from .npz file. Loaded values are reassigned to:\n",
    "        self.wih and self.who. In this case weights would be available inside the class\n",
    "        '''\n",
    "        if file_path.endswith(\".npz\"):\n",
    "            data = np.load(file_path)\n",
    "        else:\n",
    "            raise Exception(\"Accepted file should have .npz format. Current file: {}\".format(file_path))\n",
    "        self.wih, self.who = data['wih'], data['who']\n",
    "    \n",
    "    def save_weights_to_file(self, filename, current_precision=\"random\"):\n",
    "        '''\n",
    "        Saves weights to .npz file.\n",
    "        '''\n",
    "        np.savez(filename, wih=self.wih, who=self.who, precision=current_precision)\n",
    "        print('Saved weights to file: \"{}\". Specified precision: \"{}\"'.format(filename, current_precision))\n",
    "    \n",
    "    def train(self, input_list, targets_list):\n",
    "        '''\n",
    "        Trains nn. \n",
    "        :param: input_list - list of inputs. Should be regular python list. Must be normalized (/255 * 0.99) + 0.01) \n",
    "                before usage in this method\n",
    "        :targets_list: - list of target values. Should be regular python list\n",
    "        '''\n",
    "        # Convert input/target lists to NP array (matrix) and transpose those. For example:\n",
    "        '''\n",
    "        >>> l\n",
    "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "        >>> np.array(l).T\n",
    "        array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
    "        >>> np.array(l, ndmin=2)\n",
    "        array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])\n",
    "        >>> np.array(l, ndmin=2).T\n",
    "        array([[ 0],\n",
    "               [ 1],\n",
    "               [ 2],\n",
    "               [ 3],\n",
    "               [ 4],\n",
    "               [ 5],\n",
    "               [ 6],\n",
    "               [ 7],\n",
    "               [ 8],\n",
    "               [ 9],\n",
    "               [10]])\n",
    "        '''\n",
    "        inputs = np.array(input_list, ndmin=2).T\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        #  calculate signals into hidden layer. (Perform matrix multiplication)\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        #  calculate signals from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        # calculate signals from output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # calculate error on the output\n",
    "        output_errors = targets - final_outputs\n",
    "        \n",
    "        # calculate errors from hidden layer to output layer. (They shall be proportional to weights\n",
    "        # from hidden layer to output layer).\n",
    "        hidden_errors = np.dot(self.who.T, output_errors)\n",
    "        \n",
    "        # Update weights for the links between the hidden and ouput layers (backpropagation)\n",
    "                                                                                                \n",
    "                                \n",
    "        self.who += self.lr * np.dot((output_errors * final_outputs * (1 - final_outputs)), np.transpose(hidden_outputs))\n",
    "        \n",
    "        # Update weights for the links between input layer and hidden layer (backpropagation):\n",
    "        self.wih += self.lr * np.dot((hidden_errors * hidden_outputs * (1 - hidden_outputs)), np.transpose(inputs))\n",
    "        \n",
    "    def get_all_output_signals(self, inputs_list):\n",
    "        '''\n",
    "        Queries neural network with input_list\n",
    "        Note, MNIST data has to be normalized (between -1 and 1, not equal to 1 and non zero)\n",
    "        :input_list: should be regurar python list. In this case it shall be from MINST data set\n",
    "                     Must NOT include first number(actual value on image) from MINST\n",
    "        :return: list of possibilities for 10 numbers (0 to 9)\n",
    "        '''\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        signals_into_output_layer = np.dot(self.who, hidden_outputs)\n",
    "        # Return signals (probabilities) for numbers \n",
    "        return self.activation_function(signals_into_output_layer)\n",
    "    \n",
    "    def get_label_from_inputs(self, inputs_list):\n",
    "        '''\n",
    "        Returns label by obtaining maximum output signal.\n",
    "        Inputs should be between -1 and 1, not equal to 1 and non zero\n",
    "        :input_list: should be regurar python list. In this case it shall be from MINST data set. \n",
    "                     Must NOT include first number(actual value on image) from MINST\n",
    "        See get_all_output_signals for more details\n",
    "        '''\n",
    "        return np.argmax(self.get_all_output_signals(inputs_list))\n",
    "        \n",
    "    def evaluate_precision_using_mnist_file(self, file_name):\n",
    "        '''\n",
    "        Calculates precision of current Neural Network using specified MNIST file\n",
    "        Prints out the results.\n",
    "        :param: file_name - name of the csv file.\n",
    "        :return: current precision in %. For example, 23.59%\n",
    "        '''\n",
    "        with open(file_name, \"r\") as f:\n",
    "            test_data = f.readlines()\n",
    "        scorecard = []\n",
    "        for record in test_data:\n",
    "            record = record.rstrip()\n",
    "            all_values = record.split(',')\n",
    "            correct_label = int(all_values[0])\n",
    "            # normalizing, so inputs will be between -1 and 1 (/255. 255 because max value is 255 in MNIST data set), \n",
    "            # not equaling to 1 (*0.99) and non zero(+0.01)\n",
    "            # np.asfarray converts array with strings to array with floating point numbers\n",
    "            inputs = (np.asfarray(all_values[1:])/255 * 0.99) + 0.01\n",
    "            outputs = self.get_all_output_signals(inputs)\n",
    "            cur_label = self.get_label_from_inputs(inputs)\n",
    "            if cur_label == correct_label:\n",
    "                scorecard.append(1)\n",
    "            else:\n",
    "                scorecard.append(0)\n",
    "            scorecard_array = np.asarray(scorecard)\n",
    "        precision_is = (scorecard_array.sum() / scorecard_array.size)*100\n",
    "        print('Correct guess percentage: {}%'.format(precision_is))\n",
    "        print('Size of test data: {} records'.format(scorecard_array.size))\n",
    "        return precision_is\n",
    "    \n",
    "    def print_weights(self):\n",
    "        print('====weights input hidden====')        \n",
    "        print('Size: ', self.wih.size)\n",
    "        print(len(self.wih[0]))\n",
    "        print(self.wih[0])\n",
    "        print('====weights hidden output====')\n",
    "        print('Size: ', self.who[0].size)\n",
    "        print(self.who[0])\n",
    "    \n",
    "    def load_latest_weights_from_file(self):\n",
    "        '''\n",
    "        Loads weights from last created file (freshest file) in currend directory\n",
    "        '''\n",
    "        # how to get oldest file: self.chrome_extension_path = max(list_of_files, key=os.path.getctime)\n",
    "        cur_dir = os.getcwd()\n",
    "        epoch_files = glob.glob(os.path.join(cur_dir, '*.npz'))\n",
    "        file_name = max(epoch_files, key=os.path.getctime)\n",
    "        self.load_weights_from_file(file_name)\n",
    "        print(\"Loaded weights from file: {}\".format(file_name))\n",
    "    \n",
    "    def rotate_inputs(self, inputs, rotation_degrees):\n",
    "        '''\n",
    "        Rotates current nubmer by\n",
    "        :param: input_list - normalized input list\n",
    "        :param: rotation_degrees - number of degrees to rotate. Negative number - rotates left\n",
    "        :return: rotated input_list\n",
    "        '''\n",
    "        scaled_input = inputs.reshape(28, 28)\n",
    "        inputs_rotated = scipy.ndimage.rotate(scaled_input, rotation_degrees, cval=0.01, order=1, reshape=False)\n",
    "        # matplotlib.pyplot.imshow(inputs_rotated, cmap='Greys', interpolation='None')\n",
    "        inputs_flattened = inputs_rotated.flatten()\n",
    "        return inputs_flattened\n",
    "    \n",
    "    def train_several_epochs(self, file_name, number_of_epochs, rotation=True):\n",
    "        '''\n",
    "        Trains several epochs.\n",
    "        1 Epoch is 1 round of training using specified file.\n",
    "        Please note, this method rotates images +- 10 degrees. So technically total number of trained records is:\n",
    "        number-of-records-in-file * 3.\n",
    "        '''\n",
    "        total_training_time = 0\n",
    "        result_dict = {}\n",
    "        for epoch_number in range(1, number_of_epochs + 1):\n",
    "            print(\">\"*25, \"Epoch number: \", epoch_number)\n",
    "            start_epoch_time = time.time()\n",
    "            with open(file_name, 'r') as f:\n",
    "                # iterating through each line without loading it to memory (for example, f.readlines()\n",
    "                # could cause MemoryError with large amounts of data)\n",
    "                record_count = 0\n",
    "                file_content = [line for line in f]\n",
    "                shuffle(file_content)\n",
    "                for record in file_content:\n",
    "                    record = record.rstrip()\n",
    "                    all_values = record.split(',')\n",
    "                    # normalizing, so inputs will be between -1 and 1 (/255. 255 because max value is 255 in MNIST data set), \n",
    "                    # not equaling to 1 (*0.99) and non zero(+0.01)\n",
    "                    inputs = (np.asfarray(all_values[1:])/255 * 0.99) + 0.01\n",
    "                    # now we need the targets. all_values[0] represents actual number\n",
    "                    targets = np.zeros(self.onodes) + 0.01\n",
    "                    # one of targets should correspond to our number. For example, if all_values[0]=='1',\n",
    "                    # then targets = [0.01, 0.99, 0.01 ... 0.01]\n",
    "                    targets[int(all_values[0])] = 0.99\n",
    "                    n.train(inputs, targets)\n",
    "                    rotated_plus_10 = self.rotate_inputs(inputs, 10.0)     # \"inputs\" rotated plus 10 degress\n",
    "                    n.train(rotated_plus_10, targets)\n",
    "                    rotated_minus_10 = self.rotate_inputs(inputs, -10.0) # \"inputs\" rotated plus 10 degress\n",
    "                    n.train(rotated_minus_10, targets)\n",
    "                    record_count += 3 # because we trained on -10 degrees rotated, original image, +10 degrees rotation\n",
    "            end_epoch_time = time.time()\n",
    "            epoch_training_duration = end_epoch_time - start_epoch_time\n",
    "            total_training_time += epoch_training_duration\n",
    "            cur_precision = self.evaluate_precision_using_mnist_file('mnist_test.csv')\n",
    "            self.save_weights_to_file('epoch_{}_{}hidden_nodes.npz'.format(epoch_number, self.hnodes), cur_precision)\n",
    "            print('Epoch Training took: {} seconds'.format(epoch_training_duration))\n",
    "            print('Total number of records with which we tratined NN: {}'.format(record_count))\n",
    "            result_dict['epoch_{}_precision'.format(epoch_number)] = cur_precision\n",
    "        print(\"=\"*80)\n",
    "        print(\"Total duration: {} seconds\".format(total_training_time))\n",
    "        print(\"Precisions: {}\".format(result_dict))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using weights from file: /Users/bkapusta/Desktop/MachineLearning/Neural_Network/epoch_0_250hidden_nodes_normal_distribution.npz\n"
     ]
    }
   ],
   "source": [
    "n = NeuralNetwork()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n.print_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct guess percentage: 14.760000000000002%\n",
      "Size of test data: 10000 records\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.760000000000002"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.evaluate_precision_using_mnist_file(\"mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>>>>>> Epoch number:  1\n",
      "Correct guess percentage: 94.64%\n",
      "Size of test data: 10000 records\n",
      "Saved weights to file: \"epoch_1_250hidden_nodes.npz\". Specified precision: \"94.64\"\n",
      "Epoch Training took: 95.95858502388 seconds\n",
      "Total number of records with which we tratined NN: 180000\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>> Epoch number:  2\n",
      "Correct guess percentage: 96.21%\n",
      "Size of test data: 10000 records\n",
      "Saved weights to file: \"epoch_2_250hidden_nodes.npz\". Specified precision: \"96.21\"\n",
      "Epoch Training took: 94.6996841430664 seconds\n",
      "Total number of records with which we tratined NN: 180000\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>> Epoch number:  3\n",
      "Correct guess percentage: 96.78999999999999%\n",
      "Size of test data: 10000 records\n",
      "Saved weights to file: \"epoch_3_250hidden_nodes.npz\". Specified precision: \"96.78999999999999\"\n",
      "Epoch Training took: 97.21113991737366 seconds\n",
      "Total number of records with which we tratined NN: 180000\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>> Epoch number:  4\n",
      "Correct guess percentage: 97.17%\n",
      "Size of test data: 10000 records\n",
      "Saved weights to file: \"epoch_4_250hidden_nodes.npz\". Specified precision: \"97.17\"\n",
      "Epoch Training took: 96.62975120544434 seconds\n",
      "Total number of records with which we tratined NN: 180000\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>> Epoch number:  5\n",
      "Correct guess percentage: 97.50999999999999%\n",
      "Size of test data: 10000 records\n",
      "Saved weights to file: \"epoch_5_250hidden_nodes.npz\". Specified precision: \"97.50999999999999\"\n",
      "Epoch Training took: 99.19010210037231 seconds\n",
      "Total number of records with which we tratined NN: 180000\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>> Epoch number:  6\n",
      "Correct guess percentage: 97.66%\n",
      "Size of test data: 10000 records\n",
      "Saved weights to file: \"epoch_6_250hidden_nodes.npz\". Specified precision: \"97.66\"\n",
      "Epoch Training took: 104.2546238899231 seconds\n",
      "Total number of records with which we tratined NN: 180000\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>> Epoch number:  7\n",
      "Correct guess percentage: 97.76%\n",
      "Size of test data: 10000 records\n",
      "Saved weights to file: \"epoch_7_250hidden_nodes.npz\". Specified precision: \"97.76\"\n",
      "Epoch Training took: 102.47359800338745 seconds\n",
      "Total number of records with which we tratined NN: 180000\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>> Epoch number:  8\n",
      "Correct guess percentage: 97.74000000000001%\n",
      "Size of test data: 10000 records\n",
      "Saved weights to file: \"epoch_8_250hidden_nodes.npz\". Specified precision: \"97.74000000000001\"\n",
      "Epoch Training took: 92.48503017425537 seconds\n",
      "Total number of records with which we tratined NN: 180000\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>> Epoch number:  9\n",
      "Correct guess percentage: 97.77%\n",
      "Size of test data: 10000 records\n",
      "Saved weights to file: \"epoch_9_250hidden_nodes.npz\". Specified precision: \"97.77\"\n",
      "Epoch Training took: 86.61504697799683 seconds\n",
      "Total number of records with which we tratined NN: 180000\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>> Epoch number:  10\n",
      "Correct guess percentage: 97.78999999999999%\n",
      "Size of test data: 10000 records\n",
      "Saved weights to file: \"epoch_10_250hidden_nodes.npz\". Specified precision: \"97.78999999999999\"\n",
      "Epoch Training took: 1081.8909177780151 seconds\n",
      "Total number of records with which we tratined NN: 180000\n",
      "================================================================================\n",
      "Total duration: 1951.4084792137146 seconds\n",
      "Precisions: {'epoch_1_precision': 94.64, 'epoch_2_precision': 96.21, 'epoch_3_precision': 96.78999999999999, 'epoch_4_precision': 97.17, 'epoch_5_precision': 97.50999999999999, 'epoch_6_precision': 97.66, 'epoch_7_precision': 97.76, 'epoch_8_precision': 97.74000000000001, 'epoch_9_precision': 97.77, 'epoch_10_precision': 97.78999999999999}\n"
     ]
    }
   ],
   "source": [
    "n.train_several_epochs('mnist_train.csv', 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
