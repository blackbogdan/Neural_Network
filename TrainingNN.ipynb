{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "import matplotlib.pyplot\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "\n",
    "        # creating weights\n",
    "        self.wih = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "\n",
    "        self.lr = learningrate\n",
    "\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "    \n",
    "    def query(self, input_list):\n",
    "        # convert input list to 2d array\n",
    "        inputs = np.array(input_list, ndmin=2).T  # Bogdan Clarify why we need to do it\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # caclucate signals into output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        return final_outputs\n",
    "\n",
    "    def train(self, input_list, targets_list):\n",
    "        # convert inputs to 2d array\n",
    "        inputs = np.array(input_list, ndmin=2).T\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        # calculate signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        # calculate signals of outputs\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # calculating errors (target - actual)\n",
    "        output_errors = targets - final_outputs\n",
    "        hidden_errors = np.dot(self.who.T, output_errors)\n",
    "        \n",
    "        # update the weights for the links between hidden and output layers\n",
    "        self.who += self.lr * np.dot((output_errors * final_outputs * (1 - final_outputs)), np.transpose(hidden_outputs))\n",
    "        \n",
    "        # update the weights between input and hidden layers\n",
    "        self.wih += self.lr * np.dot((hidden_errors * hidden_outputs * (1 - hidden_outputs)), np.transpose(inputs))\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def print_weights(self):\n",
    "        print('====weights input hidden====')        \n",
    "        print('Size: ', self.wih.size)\n",
    "        print(len(self.wih[0]))\n",
    "        print(self.wih[0])\n",
    "        print('====weights hidden output====')\n",
    "        print('Size: ', self.who[0].size)\n",
    "        print(self.who[0])\n",
    "    \n",
    "    def save_weights_to_file(self, filename='saved_weights.npz'):\n",
    "        np.savez(filename, wih=self.wih, who=self.who)\n",
    "        print('Saved weights to file: \"{}\"'.format(filename))\n",
    "    \n",
    "    def load_weights_from_file(self, filename='saved_weights.npz'):\n",
    "        '''\n",
    "        Loads wih and who from .npz file. Loaded values are reassigned to:\n",
    "        self.wih and self.who\n",
    "        '''\n",
    "        data = np.load(filename)\n",
    "        self.wih, self.who = data['wih'], data['who']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_nodes = 784 # Because we have picture 28x28 pixels\n",
    "hidden_nodes = 100 # We can think of this number as representation of features that correspond to \n",
    "# picture of number. The more hidden_nodes more features our NN can see. If we choose smaller \"hidden_nodes\"\n",
    "# that some of features should be combined \n",
    "output_nodes = 10 # Because we have numbers from 0 to 9\n",
    "\n",
    "learning_rate = 0.3\n",
    "\n",
    "n = NeuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "# # with open('mnist_train_100.csv', 'r') as f:\n",
    "# with open('mnist_train.csv', 'r') as f:\n",
    "#         training_data_list = f.readlines()\n",
    "\n",
    "# for record in training_data_list:\n",
    "#     record = record.rstrip()\n",
    "#     all_values = record.split(',')\n",
    "#     # we need to scale inputs so, they would be small. \n",
    "#     inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "#     # now we need the targets. all_values[0] represents actual number\n",
    "#     targets = np.zeros(output_nodes) + 0.01 # created [0.01, 0.01... 0.01]\n",
    "#     # one of targets should correspond to our number. For example, if all_values[0]=='1',\n",
    "#     # then targets = [0.01, 0.99, 0.01 ... 0.01]\n",
    "#     targets[int(all_values[0])] = 0.99\n",
    "    \n",
    "#     n.train(inputs, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mnist_test.csv', 'r') as f:\n",
    "        test_data_list = f.readlines()\n",
    "# test_record = test_data_list[0].strip()\n",
    "# test_record = test_record.split(',')\n",
    "# image_array = np.asfarray(test_record[1:]).reshape(28, 28)\n",
    "# matplotlib.pyplot.imshow(image_array, cmap='Greys', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.load_weights_from_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct guess percentage:  0.9456\n",
      "Number of correctly guessed labels: 9456\n",
      "Size of test data: 10000 records\n"
     ]
    }
   ],
   "source": [
    "# Calulating correct guess percentage:\n",
    "scorecard = []\n",
    "for record in test_data_list:\n",
    "    record = record.rstrip()\n",
    "    all_values = record.split(',')\n",
    "    correct_label = int(all_values[0])\n",
    "    inputs = (np.asfarray(all_values[1:]) / 255 * 0.99) + 0.01\n",
    "    outputs = n.query(inputs)\n",
    "    label = np.argmax(outputs)\n",
    "    if label == correct_label:\n",
    "        scorecard.append(1)\n",
    "    else: \n",
    "        scorecard.append(0)\n",
    "scorecard_array = np.asarray(scorecard)\n",
    "print('Correct guess percentage: ', scorecard_array.sum() / scorecard_array.size)\n",
    "print('Number of correctly guessed labels:', scorecard_array.sum())\n",
    "print('Size of test data: {} records'.format(scorecard_array.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n.print_weights()\n",
    "# n.save_weights_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
